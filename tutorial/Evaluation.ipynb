{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Running Evaluation Metrics\n",
    "\n",
    "You can run evaluation metrics with MuSE directly, typically you already loaded your data into the relevant formats (Document, MultiDocument, or Conversation *see [Datasets.ipynb](../dataset_importer/Datasets.ipynb)*), however, just to use the evaluators, having all data as strings is enough.\n",
    "\n",
    "Currently, the supported evaluation metrics are:\n",
    "- BLEU\n",
    "- METEOR\n",
    "- ROUGE\n",
    "- Ollama *(our custom metric based on LLMs, and key-fact extraction)*\n",
    "\n",
    "These can be found in the `evaluation` module of the `muse` package, and should be accessed via `resolve_evaluator` function."
   ],
   "id": "efcf69a92671ef5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Classic Metrics (BLEU, METEOR, ROUGE)\n",
    "\n",
    "These metrics all require a reference summary in order to compare the generated summary to. The reference summary, and the generated summary should be passed as strings to the `evaluate` method of the evaluator."
   ],
   "id": "248fc1353109eba4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from muse.evaluation.resolver import resolve_evaluator\n",
    "\n",
    "reference = \"Summarization systems can be evaluated in many ways, including with metrics like BLEU, METEOR, and ROUGE.\"\n",
    "generated = \"Summarization systems can be evaluated with BLEU, METEOR, and ROUGE.\"\n",
    "\n",
    "# Resolve the evaluator you wish to use\n",
    "bleu = resolve_evaluator(\"BLEU\")\n",
    "meteor = resolve_evaluator(\"METEOR\")\n",
    "rouge = resolve_evaluator(\"ROUGE\")\n",
    "# Evaluate the generated summary\n",
    "bleu_score = bleu.evaluate([generated], reference_summary=[reference])\n",
    "meteor_score = meteor.evaluate([generated], reference_summary=[reference])\n",
    "rouge_score = rouge.evaluate([generated], reference_summary=[reference])\n",
    "\n",
    "# Print the scores\n",
    "print(f\"BLEU: {bleu_score}\")\n",
    "print(f\"METEOR: {meteor_score}\")\n",
    "print(f\"ROUGE: {rouge_score}\")"
   ],
   "id": "b2f033e7f5064abf",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ollama Metric\n",
    "\n",
    "The Ollama metric is a custom metric we developed that uses a language model to evaluate the quality of a summary. It also uses key-fact extraction to ensure that the summary is factually accurate. The metric can use either a reference summary, or the source document to evaluate the generated summary.\n",
    "\n",
    "It also takes a set of options, these can be checked with the `valid_options` method on the function:"
   ],
   "id": "cc3f5e44ce493e4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from muse.evaluation.llm.ollama_metric import OllamaMetric\n",
    "\n",
    "OllamaMetric.__init__.valid_options()"
   ],
   "id": "f8001c56970cd5aa",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These options can be left to their default in most cases, but otherwise, are passed using the `options` parameter in the `resolve_evaluator` function, or when using the `cli` via the `--config` parameter.\n",
    "\n",
    "We also must ensure `Ollama` is installed and running before we can use this metric. Please see [Ollama](https://ollama.com/) for information on how to install and run the service.\n",
    "\n",
    "Once this is ready, we can do the following:"
   ],
   "id": "5b236d067bb44f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from muse.evaluation.resolver import resolve_evaluator\n",
    "\n",
    "reference = \"Summarization systems can be evaluated in many ways, including with metrics like BLEU, METEOR, and ROUGE.\"\n",
    "generated = \"Summarization systems can be evaluated with BLEU, METEOR, and ROUGE.\"\n",
    "\n",
    "# Resolve the evaluator you wish to use\n",
    "ollama = resolve_evaluator(\"Ollama\")\n",
    "# Evaluate the generated summary\n",
    "ollama_score = ollama.evaluate([generated], reference_text=[reference])\n",
    "\n",
    "# Print the score\n",
    "print(f\"Ollama: {ollama_score}\")"
   ],
   "id": "c812e0c814a69fc5",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
